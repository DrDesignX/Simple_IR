{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl # type: ignore\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DATA_URL` contains the file path to the Cranfield collection documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL  = \"../data/documents/Cranfield collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loads stopwords from a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as stop_file:\n",
    "            return stop_file.read().split(\",\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return []\n",
    "\n",
    "stopwords_file_path = \"../data/stopword/words.txt\"\n",
    "stopwords = load_stopwords(stopwords_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Stemming\n",
    "\n",
    "tokenizes the input text, removes stopwords, and then stems each token.\n",
    "Returns:\n",
    "A string containing the stemmed tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def tokenization_and_stemming(text):\n",
    "    tokens = per_tokenize(text)\n",
    "    filtered_tokens = remove_stopwords(tokens)\n",
    "    stems = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def per_tokenize(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    # text = ''.join(char if char.isalpha() else ' ' for char in text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parses text documents located in the specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(directory=DATA_URL):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"cran.all\") or filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), \"r\") as file:\n",
    "            text = file.read()\n",
    "            documents = text.split(\".I\")\n",
    "    return documents[1:]\n",
    "documents = parse_text()\n",
    "# documents.remove(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stem the data and then tokenize the first document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_data = []\n",
    "for document in documents:\n",
    "    stem_data.append(tokenization_and_stemming(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creates a list of word count dictionaries from a list of stemmed documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_count_list():\n",
    "    word_count_list = []\n",
    "    for document in stem_data:\n",
    "        word_count = {}\n",
    "        tokens = document.split()\n",
    "        for token in tokens:\n",
    "            token = token.strip('.,?!\";:')\n",
    "            word_count[token] = word_count.get(token, 0) + 1\n",
    "        if word_count:\n",
    "            del word_count[next(iter(word_count))]\n",
    "        word_count_list.append(word_count)\n",
    "    return word_count_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "takes a list of word count tuples and returns a set of unique words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(word_count_list):\n",
    "    unique_words = set()\n",
    "    for word_count in word_count_list:\n",
    "        unique_words.update(word_count.keys())\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creates a word count matrix from a list of word count dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_to_excel(word_count_list, file_name):\n",
    "#         wb = openpyxl.Workbook()\n",
    "#         sheet = wb.active\n",
    "#         sheet.title = \"Word Count Data\"\n",
    "#         all_words = set()\n",
    "#         for word_count in word_count_list:\n",
    "#             all_words.update(word_count.keys())\n",
    "\n",
    "#         all_words = sorted(all_words)\n",
    "#         sheet.cell(row=1, column=1).value = Words\n",
    "#         for col, document in enumerate(range(2, len(word_count_list) + 2), start=2):\n",
    "#             sheet.cell(row=1, column=col).value = f\"Document {document - 1}\"\n",
    "#         for row, word in enumerate(all_words, start=2):\n",
    "#             sheet.cell(row=row, column=1).value = word\n",
    "#             for col, word_count in enumerate(word_count_list, start=2):\n",
    "#                 count = word_count.get(word, 0)  # If word not found, default to 0\n",
    "#                 sheet.cell(row=row, column=col).value = count\n",
    "\n",
    "#         wb.save(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrix(word_doc_list):\n",
    "    # Create a set of all unique words across all documents\n",
    "    all_words = set(unique_words(word_doc_list))\n",
    "    # Determine the dimensions of the matrix\n",
    "    num_rows = len(all_words) + 1  # +1 for the header row\n",
    "    num_cols = len(word_doc_list) + 1  # +1 for the \"Words\" column\n",
    "    # Create the matrix\n",
    "    matrix = [[0] * num_cols for _ in range(num_rows)]\n",
    "    # Populate the header row\n",
    "    matrix[0][0] = \"Words\"\n",
    "    for col in range(1, num_cols):\n",
    "        matrix[0][col] = f\"Document {col}\"\n",
    "    # Populate the word counts\n",
    "    for row, word in enumerate(all_words, start=1):\n",
    "        matrix[row][0] = word\n",
    "        for col, word_count in enumerate(word_doc_list, start=1):\n",
    "            count = word_count.get(word, 0)\n",
    "            matrix[row][col] = count\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Word Count Matrix and DataFrame\n",
    "\n",
    "generate a word count matrix and create a DataFrame from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in all documents: 6315\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Document 1</th>\n",
       "      <th>Document 2</th>\n",
       "      <th>Document 3</th>\n",
       "      <th>Document 4</th>\n",
       "      <th>Document 5</th>\n",
       "      <th>Document 6</th>\n",
       "      <th>Document 7</th>\n",
       "      <th>Document 8</th>\n",
       "      <th>Document 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Document 1391</th>\n",
       "      <th>Document 1392</th>\n",
       "      <th>Document 1393</th>\n",
       "      <th>Document 1394</th>\n",
       "      <th>Document 1395</th>\n",
       "      <th>Document 1396</th>\n",
       "      <th>Document 1397</th>\n",
       "      <th>Document 1398</th>\n",
       "      <th>Document 1399</th>\n",
       "      <th>Document 1400</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>experiment</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Words  Document 1  Document 2  Document 3  Document 4  Document 5  \\\n",
       "1595  experiment           3           0           0           0           0   \n",
       "\n",
       "      Document 6  Document 7  Document 8  Document 9  ...  Document 1391  \\\n",
       "1595           0           0           0           0  ...              0   \n",
       "\n",
       "      Document 1392  Document 1393  Document 1394  Document 1395  \\\n",
       "1595              1              0              0              0   \n",
       "\n",
       "      Document 1396  Document 1397  Document 1398  Document 1399  \\\n",
       "1595              1              1              0              0   \n",
       "\n",
       "      Document 1400  \n",
       "1595              0  \n",
       "\n",
       "[1 rows x 1401 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_list = create_word_count_list()\n",
    "unique_words_list = unique_words(word_count_list)\n",
    "word_tf_matrix = generate_matrix(word_count_list)\n",
    "print(\"Number of unique words in all documents:\", len(unique_words_list))\n",
    "df = pd.DataFrame(word_tf_matrix[1:], columns=word_tf_matrix[0])\n",
    "filtered_df = df[df['Words'].str.contains('experiment')]\n",
    "# filtered_df.to_excel(\"data.xlsx\", index=False)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_log_tf(tf):\n",
    "    if tf == 0:\n",
    "        return 0  # Logarithm of 0 is undefined\n",
    "    else:\n",
    "        return 1 + math.log(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(word_tf_matrix)):\n",
    "    for j in range(1,len(word_tf_matrix[i])):\n",
    "        tf = word_tf_matrix[i][j]\n",
    "        word_tf_matrix[i][j] = calculate_log_tf(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
