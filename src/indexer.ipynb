{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL  = '/Users/mohamad/Documents/GitHub/BrainQuest/data/documents/Cranfield collection/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Stemmer def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens\n",
    "\n",
    "def stem_word(token):\n",
    "    return stemmer.stem(token)\n",
    "\n",
    "# Test\n",
    "print(tokens)\n",
    "stemmed_tokens = [stem_word(token) for token in tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def parser_text(File=DATA_URL):\n",
    "    documents = []\n",
    "    for filename in os.listdir(File):\n",
    "        if not filename.startswith(\"cran.all\") or filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(File, filename), \"r\") as file:\n",
    "            current_document = {}\n",
    "            current_text = []\n",
    "            stratDoc = True\n",
    "            i = 0\n",
    "            for line in file:\n",
    "                while stratDoc == False:\n",
    "                    current_line = line.strip()\n",
    "                    if current_line.startswith(\".I\"):\n",
    "                        stratDoc == \n",
    "\n",
    "\n",
    "    data = {\n",
    "        \"title\": title,\n",
    "        \"author\": author,\n",
    "        \"bibliography\": bibliography,\n",
    "        \"text\": text,\n",
    "\n",
    "    }\n",
    "    return documents\n",
    "\n",
    "result = parser_text(DATA_URL)\n",
    "\n",
    "# Write the result to a JSON file\n",
    "with open('result.json', 'w') as file:\n",
    "    json.dump(result, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change stop words to list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listmaker():\n",
    "    stop_file = open(\"/Users/mohamad/Documents/GitHub/BrainQuest/data/stopword/words.txt\", \"r\")\n",
    "    try:\n",
    "        content = stop_file.read()\n",
    "        stopwords = content.split(\",\")\n",
    "    finally:\n",
    "        stop_file.close()\n",
    "    return stopwords\n",
    "\n",
    "stopwords = listmaker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove StopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text, stemmer):\n",
    "    tokens = tokenize(text)\n",
    "    filtered_tokens = remove_stopwords(tokens)\n",
    "    stems = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_cranfield():\n",
    "    documents = parser_text()\n",
    "    print(documents)\n",
    "    for document_data in documents:\n",
    "        document = Document(\n",
    "            id=document_data[\"id\"],\n",
    "            title=document_data.get(\"title\", \"\")\n",
    "            if document_data.get(\"title\", \"\")\n",
    "            else \"\",\n",
    "            author=document_data.get(\"author\", \"\")\n",
    "            if document_data.get(\"author\", \"\")\n",
    "            else \"\",\n",
    "            bibliography=document_data.get(\"bibliography\", \"\")\n",
    "            if document_data.get(\"bibliography\", \"\")\n",
    "            else \"\",\n",
    "            text=document_data[\"text\"] if document_data.get(\"text\") else \"\",\n",
    "        )\n",
    "        document.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
