{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import openpyxl # type: ignore\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL  = '/Users/mohamad/Documents/GitHub/BrainQuest/data/documents/Cranfield collection/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as stop_file:\n",
    "            content = stop_file.read()\n",
    "            stopwords = content.split(\",\")\n",
    "            return stopwords\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return []\n",
    "\n",
    "stopwords_file_path = \"/Users/mohamad/Documents/GitHub/BrainQuest/data/stopword/words.txt\"\n",
    "stopwords = load_stopwords(stopwords_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_and_stemming(text):\n",
    "    tokens = per_tokenize(text)\n",
    "    filtered_tokens = remove_stopwords(tokens)\n",
    "    stems = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def per_tokenize(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def parse_text(directory=DATA_URL):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"cran.all\") or filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), \"r\") as file:\n",
    "            text = file.read()\n",
    "            documents = text.split(\".I\")\n",
    "    return documents\n",
    "documents = parse_text()\n",
    "documents.remove(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_data = []\n",
    "for document in documents:\n",
    "    stem_data.append(tokenization_and_stemming(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_count_list():\n",
    "    word_count_list = []\n",
    "    for document in stem_data:\n",
    "        word_count = {}\n",
    "        tokens = document.split()\n",
    "        for token in tokens:\n",
    "            token = token.strip('.,?!\";:')\n",
    "            if token in word_count:\n",
    "                word_count[token] += 1\n",
    "            else:\n",
    "                word_count[token] = 1\n",
    "        if word_count:\n",
    "            del word_count[next(iter(word_count))]\n",
    "        word_count_list.append(word_count)\n",
    "    return word_count_list\n",
    "\n",
    "word_count_list = create_word_count_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dic(word_count_list):\n",
    "    for i, word_count in enumerate(word_count_list, start=1):\n",
    "        print(f\"\\nDocument {i}:\")\n",
    "        for word, count in word_count.items():\n",
    "            print(f\"{word}: {count}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(word_count_list):\n",
    "    unique_words = []\n",
    "    for word_count in word_count_list:\n",
    "        unique_words.extend(word_count)\n",
    "    return set(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "# def write_to_excel(word_count_list, file_name):\n",
    "#     wb = openpyxl.Workbook()\n",
    "#     sheet = wb.active\n",
    "#     sheet.title = \"Word Count Data\"\n",
    "\n",
    "#     # Extracting all unique words\n",
    "#     all_words = set()\n",
    "#     for word_count in word_count_list:\n",
    "#         all_words.update(word_count.keys())\n",
    "#     all_words = sorted(all_words)\n",
    "\n",
    "#     # Writing header row (document names)\n",
    "#     sheet.cell(row=1, column=1).value = \"Words\"\n",
    "#     for col, document in enumerate(range(2, len(word_count_list) + 2), start=2):\n",
    "#         sheet.cell(row=1, column=col).value = f\"Document {document - 1}\"\n",
    "\n",
    "#     # Writing data rows (word counts) in batches\n",
    "#     batch_size = 10000  # Adjust batch size based on system memory\n",
    "#     num_batches = (len(all_words) + batch_size - 1) // batch_size\n",
    "#     for batch_index in range(num_batches):\n",
    "#         start_index = batch_index * batch_size\n",
    "#         end_index = min(start_index + batch_size, len(all_words))\n",
    "#         for row, word in enumerate(all_words[start_index:end_index], start=2):\n",
    "#             sheet.cell(row=row, column=1).value = word\n",
    "#             for col, word_count in enumerate(word_count_list, start=2):\n",
    "#                 count = word_count.get(word, 0)  # If word not found, default to 0\n",
    "#                 sheet.cell(row=row, column=col).value = count\n",
    "\n",
    "#     wb.save(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_count_list = create_word_count_list()\n",
    "unique_words_list = unique_words(word_count_list)\n",
    "len(unique_words_list)\n",
    "print_dic(word_count_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dic(word_count_list)\n",
    "# write_to_excel(word_count_list, \"word_count_data.xlsx\")\n",
    "word_count_list[0]\n",
    "print(\"Number of unique words in all documents:\", len(unique_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_excel('/Users/mohamad/Documents/GitHub/BrainQuest/src/word_count_data.xlsx')\n",
    "# print(df['Words'].head(100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
