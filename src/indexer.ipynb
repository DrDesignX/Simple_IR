{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import openpyxl # type: ignore\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL  = \"../data/documents/Cranfield collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as stop_file:\n",
    "            content = stop_file.read()\n",
    "            stopwords = content.split(\",\")\n",
    "            return stopwords\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return []\n",
    "\n",
    "stopwords_file_path = \"../data/stopword/words.txt\"\n",
    "stopwords = load_stopwords(stopwords_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_and_stemming(text):\n",
    "    tokens = per_tokenize(text)\n",
    "    filtered_tokens = remove_stopwords(tokens)\n",
    "    stems = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def per_tokenize(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def parse_text(directory=DATA_URL):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.startswith(\"cran.all\") or filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), \"r\") as file:\n",
    "            text = file.read()\n",
    "            documents = text.split(\".I\")\n",
    "    return documents\n",
    "documents = parse_text()\n",
    "documents.remove(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'experiment',\n",
       " 'investig',\n",
       " 'aerodynam',\n",
       " 'wing',\n",
       " 'slipstream',\n",
       " 'brenckmanm',\n",
       " 'sc',\n",
       " '25',\n",
       " '1958',\n",
       " '324',\n",
       " 'experiment',\n",
       " 'investig',\n",
       " 'aerodynam',\n",
       " 'wing',\n",
       " 'slipstream',\n",
       " 'experiment',\n",
       " 'studi',\n",
       " 'wing',\n",
       " 'propel',\n",
       " 'slipstream',\n",
       " 'order',\n",
       " 'determin',\n",
       " 'spanwis',\n",
       " 'distribut',\n",
       " 'lift',\n",
       " 'increas',\n",
       " 'slipstream',\n",
       " 'angl',\n",
       " 'attack',\n",
       " 'wing',\n",
       " 'free',\n",
       " 'stream',\n",
       " 'slipstream',\n",
       " 'veloc',\n",
       " 'ratio',\n",
       " 'intend',\n",
       " 'evalu',\n",
       " 'basi',\n",
       " 'theoret',\n",
       " 'treatment',\n",
       " 'problem',\n",
       " 'compar',\n",
       " 'span',\n",
       " 'load',\n",
       " 'curv',\n",
       " 'support',\n",
       " 'evid',\n",
       " 'substanti',\n",
       " 'lift',\n",
       " 'increment',\n",
       " 'produc',\n",
       " 'slipstream',\n",
       " 'destal',\n",
       " 'boundarylayercontrol',\n",
       " 'integr',\n",
       " 'remain',\n",
       " 'lift',\n",
       " 'increment',\n",
       " 'subtract',\n",
       " 'destal',\n",
       " 'lift',\n",
       " 'agre',\n",
       " 'potenti',\n",
       " 'flow',\n",
       " 'theori',\n",
       " 'empir',\n",
       " 'evalu',\n",
       " 'destal',\n",
       " 'effect',\n",
       " 'specif',\n",
       " 'configur',\n",
       " 'experi']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_data = []\n",
    "for document in documents:\n",
    "    stem_data.append(tokenization_and_stemming(document))\n",
    "per_tokenize(stem_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_count_list():\n",
    "    word_count_list = []\n",
    "    for document in stem_data:\n",
    "        word_count = {}\n",
    "        tokens = document.split()\n",
    "        for token in tokens:\n",
    "            token = token.strip('.,?!\";:')\n",
    "            if token in word_count:\n",
    "                word_count[token] += 1\n",
    "            else:\n",
    "                word_count[token] = 1\n",
    "        if word_count:\n",
    "            del word_count[next(iter(word_count))]\n",
    "        word_count_list.append(word_count)\n",
    "    return word_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(word_count_list):\n",
    "    unique_words = []\n",
    "    for word_count in word_count_list:\n",
    "        unique_words.extend(word_count)\n",
    "    return set(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "def write_to_excel(word_count_list, file_name):\n",
    "    wb = openpyxl.Workbook()\n",
    "    sheet = wb.active\n",
    "    sheet.title = \"Word Count Data\"\n",
    "    all_words = set()\n",
    "    for word_count in word_count_list:\n",
    "        all_words.update(word_count.keys())\n",
    "    all_words = sorted(all_words)\n",
    "    sheet.cell(row=1, column=1).value = \"Words\"\n",
    "    for col, document in enumerate(range(2, len(word_count_list) + 2), start=2):\n",
    "        sheet.cell(row=1, column=col).value = f\"Document {document - 1}\"\n",
    "    for row, word in enumerate(all_words, start=2):\n",
    "        sheet.cell(row=row, column=1).value = word\n",
    "        for col, word_count in enumerate(word_count_list, start=2):\n",
    "            count = word_count.get(word, 0)  # If word not found, default to 0\n",
    "            sheet.cell(row=row, column=col).value = count\n",
    "\n",
    "    wb.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "experiment: 3\n",
      "investig: 2\n",
      "aerodynam: 2\n",
      "wing: 4\n",
      "slipstream: 6\n",
      "brenckmanm: 1\n",
      "sc: 1\n",
      "25: 1\n",
      "1958: 1\n",
      "324: 1\n",
      "studi: 1\n",
      "propel: 1\n",
      "order: 1\n",
      "determin: 1\n",
      "spanwis: 1\n",
      "distribut: 1\n",
      "lift: 4\n",
      "increas: 1\n",
      "angl: 1\n",
      "attack: 1\n",
      "free: 1\n",
      "stream: 1\n",
      "veloc: 1\n",
      "ratio: 1\n",
      "intend: 1\n",
      "evalu: 2\n",
      "basi: 1\n",
      "theoret: 1\n",
      "treatment: 1\n",
      "problem: 1\n",
      "compar: 1\n",
      "span: 1\n",
      "load: 1\n",
      "curv: 1\n",
      "support: 1\n",
      "evid: 1\n",
      "substanti: 1\n",
      "increment: 2\n",
      "produc: 1\n",
      "destal: 3\n",
      "boundarylayercontrol: 1\n",
      "integr: 1\n",
      "remain: 1\n",
      "subtract: 1\n",
      "agre: 1\n",
      "potenti: 1\n",
      "flow: 1\n",
      "theori: 1\n",
      "empir: 1\n",
      "effect: 1\n",
      "specif: 1\n",
      "configur: 1\n",
      "experi: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_count_list = create_word_count_list()\n",
    "unique_words_list = unique_words(word_count_list)\n",
    "print_dic(word_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "experiment: 3\n",
      "investig: 2\n",
      "aerodynam: 2\n",
      "wing: 4\n",
      "slipstream: 6\n",
      "brenckmanm: 1\n",
      "sc: 1\n",
      "25: 1\n",
      "1958: 1\n",
      "324: 1\n",
      "studi: 1\n",
      "propel: 1\n",
      "order: 1\n",
      "determin: 1\n",
      "spanwis: 1\n",
      "distribut: 1\n",
      "lift: 4\n",
      "increas: 1\n",
      "angl: 1\n",
      "attack: 1\n",
      "free: 1\n",
      "stream: 1\n",
      "veloc: 1\n",
      "ratio: 1\n",
      "intend: 1\n",
      "evalu: 2\n",
      "basi: 1\n",
      "theoret: 1\n",
      "treatment: 1\n",
      "problem: 1\n",
      "compar: 1\n",
      "span: 1\n",
      "load: 1\n",
      "curv: 1\n",
      "support: 1\n",
      "evid: 1\n",
      "substanti: 1\n",
      "increment: 2\n",
      "produc: 1\n",
      "destal: 3\n",
      "boundarylayercontrol: 1\n",
      "integr: 1\n",
      "remain: 1\n",
      "subtract: 1\n",
      "agre: 1\n",
      "potenti: 1\n",
      "flow: 1\n",
      "theori: 1\n",
      "empir: 1\n",
      "effect: 1\n",
      "specif: 1\n",
      "configur: 1\n",
      "experi: 1\n"
     ]
    }
   ],
   "source": [
    "print_dic(word_count_list)\n",
    "write_to_excel(word_count_list, \"word_count_data.xlsx\")\n",
    "word_count_list[0]\n",
    "print(\"Number of unique words in all documents:\", len(unique_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in all documents: 8626\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words in all documents:\", len(unique_words_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
